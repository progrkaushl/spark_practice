{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4cdcde-506f-4c7a-ae2a-696d7139ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c41c58-576c-408a-bd95-4788e4388ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 06:40:54 WARN Utils: Your hostname, Kaushl resolves to a loopback address: 127.0.1.1; using 172.31.15.1 instead (on interface eth0)\n",
      "22/06/09 06:40:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/09 06:41:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession and sparkcontext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName('Firstprogram')\\\n",
    "                    .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b10b23-d9bb-48bf-8075-d51fab616b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WordCount Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04b4f0-0b6a-44c4-92c9-274f26768cb5",
   "metadata": {},
   "source": [
    "Our requirement is to write a small program to display the number of occurrence of each word in the given input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e210f92d-d403-49f6-a4e8-85e24b889e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file to create RDD\n",
    "text_file = sc.textFile('data/wordCountTest.txt', minPartitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6782bbe-2b69-4756-9245-a55a18dd8daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print number of partitions\n",
    "text_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc16f023-a6ec-4776-96ff-189a68234f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of words\n",
    "word_counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word.lower(), 1)).reduceByKey(lambda x,y: x+y)\n",
    "word_counts_sorted = word_counts.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6e26e4-8b2f-4a82-8d50-a54a5046b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 8\n",
      "to: 7\n",
      "in: 5\n",
      "with: 5\n",
      ": 4\n",
      "and: 4\n",
      "jupyter: 3\n",
      "start: 3\n",
      "pyspark: 3\n",
      "our: 3\n"
     ]
    }
   ],
   "source": [
    "# Printing each word with its respective count\n",
    "output_rdd = word_counts_sorted.take(10)\n",
    "for word, count in output_rdd:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a048f5-b81a-48e6-ac07-0b29ce214647",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Add Preceding Zero to Column in Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205e6d9-8169-4a2f-83ed-104be097f25f",
   "metadata": {},
   "source": [
    "Consider we have a input Spark dataframe as shown in the below figure with the couple of column Name and Score. \n",
    "\n",
    "\n",
    "Our requirement is to:\n",
    "Add Leading Zero to the column Score and make it column of three digit as shown in the below output Spark dataframe. \n",
    "- for example, convert 89 to 089 and \n",
    "- convert 8 into 008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b456e08-cf1f-49d3-a144-db95f67e44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|score|\n",
      "+-----+-----+\n",
      "| Babu|   20|\n",
      "| Raja|    8|\n",
      "| Mani|   75|\n",
      "|Kalam|  100|\n",
      "| Zoin|    7|\n",
      "|  Kal|   53|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create input Spark Dataframe\n",
    "list_data=[[\"Babu\",20],[\"Raja\",8],[\"Mani\",75],[\"Kalam\",100],[\"Zoin\",7],[\"Kal\",53]]\n",
    "df1=spark.createDataFrame(list_data,[\"name\",\"score\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91d90c-8245-4ac3-b6b2-c29beac6687c",
   "metadata": {},
   "source": [
    "#### Using format_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514fe52-e450-4b53-ae68-0012a1b7227e",
   "metadata": {},
   "source": [
    "Format String basically formats the given value in the printf-style and returns the resultant value as a string output. One can import the Spark SQL function format_string and use it to add the leading zeros or for zero padding to the column in Spark dataframe.\n",
    "\n",
    "**General Syntax**:\n",
    "format_string(format,*cols)\n",
    "\n",
    "**Parameters**:\n",
    "* format (str): string of embedded format tags like %d %s used to format result column value \n",
    "* cols (column or str): Either a single column or multiple columns in Spark dataframe to be formatted\n",
    "* Coming back to our use-case, let us see the code snippet to solve our problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96f6c6c8-3f43-4a94-b6d7-c7b73e462365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63941d7d-3b61-4cc9-866b-66bebc5d5460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+\n",
      "| name|score|score_000|\n",
      "+-----+-----+---------+\n",
      "| Babu|   20|      020|\n",
      "| Raja|    8|      008|\n",
      "| Mani|   75|      075|\n",
      "|Kalam|  100|      100|\n",
      "| Zoin|    7|      007|\n",
      "|  Kal|   53|      053|\n",
      "+-----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"score_000\", format_string(\"%03d\", \"score\")) \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81d4a6-c775-4bed-9d2b-99feea83ad10",
   "metadata": {},
   "source": [
    "#### Using lpad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26dcb9e-b62a-44ae-9efc-0376c77df228",
   "metadata": {},
   "source": [
    "LPAD simply pads a string column to the with of the specified length.\n",
    "\n",
    "**General Syntax**:\n",
    "lpad(col,len,pad)\n",
    "\n",
    "**Parameter**:\n",
    "* col - Column or string to be padded\n",
    "* len - Length of the resultant column after applying padding\n",
    "* pad - character to pad (like 0 or # etc.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "071e9888-9ec9-4951-999b-90457d77e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ecef3f1-2841-4a53-959d-66ced8f87357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+\n",
      "| name|score|score_000|\n",
      "+-----+-----+---------+\n",
      "| Babu|   20|      020|\n",
      "| Raja|    8|      008|\n",
      "| Mani|   75|      075|\n",
      "|Kalam|  100|      100|\n",
      "| Zoin|    7|      007|\n",
      "|  Kal|   53|      053|\n",
      "+-----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"score_000\", lpad(\"score\", 3, \"0\"))                 # Note that we are using string 0\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f51caa-16bd-48f6-96d7-b93ac78d2b00",
   "metadata": {},
   "source": [
    "#### Using concat() + substr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93f53c59-1287-4917-85ea-d85c8816dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,substring,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f5bae2c-d1c9-451d-8487-fae782810145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+\n",
      "| name|score|score_000|\n",
      "+-----+-----+---------+\n",
      "| Babu|   20|     0020|\n",
      "| Raja|    8|      008|\n",
      "| Mani|   75|     0075|\n",
      "|Kalam|  100|    00100|\n",
      "| Zoin|    7|      007|\n",
      "|  Kal|   53|     0053|\n",
      "+-----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"score_000\", concat(lit(\"00\"), \"score\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07906c99-8c09-4f81-b3bb-5cc7dba7f2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+\n",
      "| name|score|score_000|\n",
      "+-----+-----+---------+\n",
      "| Babu|   20|      020|\n",
      "| Raja|    8|      008|\n",
      "| Mani|   75|      075|\n",
      "|Kalam|  100|      100|\n",
      "| Zoin|    7|      007|\n",
      "|  Kal|   53|      053|\n",
      "+-----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.withColumn(\"score_000\", substring(\"score_000\", -3, 3))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85280ca-d715-4aea-a962-0ef3326b8108",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Replace a String in Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab020aa0-1cc2-463e-9272-b7318daa7cc2",
   "metadata": {},
   "source": [
    "Consider we have a dataframe with columns as shown in the below figure (Input_DF). Our requirement is to replace the string value Checking in column called Card_type to Cash. The output that we are expected to workout is shown in the below figure for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "506101d7-6df1-44f8-a08f-cc5a9a69318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkFiles to add file from url to Spark\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed5e2021-4ec8-46e7-811f-1e67ebb9fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_transactions_csv = \"https://raw.githubusercontent.com/azar-s91/dataset/master/personal_transactions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e44bec3-79dc-4f74-8b81-4e29a23de78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If file is small then we can add online file to SparkFiles\n",
    "spark.sparkContext.addFile(personal_transactions_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "539902cd-14fd-4f75-b925-79a64bce79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use SparkFiles.get(file_name) to get file downlaoded file\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(SparkFiles.get(\"personal_transactions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dcd95df-fe19-44e1-a4db-234d039cb1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|Customer_No|    Card_type|    Date|           Category|Transaction Type| Amount|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|    1000501|Platinum Card|1/1/2018|           Shopping|           debit|  11.11|\n",
      "|    1000501|     Checking|1/2/2018|    Mortgage & Rent|           debit|1247.44|\n",
      "|    1000501|  Silver Card|1/2/2018|        Restaurants|           debit|  24.22|\n",
      "|    1000501|Platinum Card|1/3/2018|Credit Card Payment|          credit|2298.09|\n",
      "|    1000501|Platinum Card|1/4/2018|      Movies & DVDs|           debit|  11.76|\n",
      "|    1000501|  Silver Card|1/5/2018|        Restaurants|           debit|  25.85|\n",
      "|    1000501|  Silver Card|1/6/2018|   Home Improvement|           debit|  18.45|\n",
      "|    1000501|     Checking|1/8/2018|          Utilities|           debit|     45|\n",
      "|    1000501|  Silver Card|1/8/2018|   Home Improvement|           debit|  15.38|\n",
      "|    1000501|Platinum Card|1/9/2018|              Music|           debit|  10.69|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d4931-7ed3-474d-b659-9ad44ed4ada5",
   "metadata": {},
   "source": [
    "#### Method 1: Using `na.replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57069552-d552-489a-9b03-aeb76fbcdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.na.replace(\"Checking\", \"Cash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25892fbd-fb19-44f4-a0d8-448e3766bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|Customer_No|    Card_type|    Date|           Category|Transaction Type| Amount|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "|    1000501|Platinum Card|1/1/2018|           Shopping|           debit|  11.11|\n",
      "|    1000501|         Cash|1/2/2018|    Mortgage & Rent|           debit|1247.44|\n",
      "|    1000501|  Silver Card|1/2/2018|        Restaurants|           debit|  24.22|\n",
      "|    1000501|Platinum Card|1/3/2018|Credit Card Payment|          credit|2298.09|\n",
      "|    1000501|Platinum Card|1/4/2018|      Movies & DVDs|           debit|  11.76|\n",
      "|    1000501|  Silver Card|1/5/2018|        Restaurants|           debit|  25.85|\n",
      "|    1000501|  Silver Card|1/6/2018|   Home Improvement|           debit|  18.45|\n",
      "|    1000501|         Cash|1/8/2018|          Utilities|           debit|     45|\n",
      "|    1000501|  Silver Card|1/8/2018|   Home Improvement|           debit|  15.38|\n",
      "|    1000501|Platinum Card|1/9/2018|              Music|           debit|  10.69|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff3252-bb30-48e7-8768-63838e5109cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Method 2: Using `regexp_replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa3cd0cb-e195-4c17-a8ac-1452445ed7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a486173c-b23c-4db5-96d5-cea8c6d8ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"Card_type_repl\", regexp_replace(\"Card_type\", \"Checking\", \"Cash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe00a617-187d-4c15-8b0b-9322a66526a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "|Customer_No|    Card_type|    Date|           Category|Transaction Type| Amount|Card_type_repl|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "|    1000501|Platinum Card|1/1/2018|           Shopping|           debit|  11.11| Platinum Card|\n",
      "|    1000501|     Checking|1/2/2018|    Mortgage & Rent|           debit|1247.44|          Cash|\n",
      "|    1000501|  Silver Card|1/2/2018|        Restaurants|           debit|  24.22|   Silver Card|\n",
      "|    1000501|Platinum Card|1/3/2018|Credit Card Payment|          credit|2298.09| Platinum Card|\n",
      "|    1000501|Platinum Card|1/4/2018|      Movies & DVDs|           debit|  11.76| Platinum Card|\n",
      "|    1000501|  Silver Card|1/5/2018|        Restaurants|           debit|  25.85|   Silver Card|\n",
      "|    1000501|  Silver Card|1/6/2018|   Home Improvement|           debit|  18.45|   Silver Card|\n",
      "|    1000501|     Checking|1/8/2018|          Utilities|           debit|     45|          Cash|\n",
      "|    1000501|  Silver Card|1/8/2018|   Home Improvement|           debit|  15.38|   Silver Card|\n",
      "|    1000501|Platinum Card|1/9/2018|              Music|           debit|  10.69| Platinum Card|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abb7e7-4bce-4d2c-8fa4-ddc6c1fc16a6",
   "metadata": {},
   "source": [
    "#### Method 3: Using Case When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03b58e4f-9886-4dff-9da5-fc2c25e1c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca3b15b9-d5ba-46c7-9e31-36e39dc979c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"Card_type_repl\", when(col(\"Card_type\").rlike(\"Checking\"), lit(\"Cash\")).otherwise(col(\"Card_type\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3035848d-5421-417b-b51e-dc338ab20686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "|Customer_No|    Card_type|    Date|           Category|Transaction Type| Amount|Card_type_repl|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "|    1000501|Platinum Card|1/1/2018|           Shopping|           debit|  11.11| Platinum Card|\n",
      "|    1000501|     Checking|1/2/2018|    Mortgage & Rent|           debit|1247.44|          Cash|\n",
      "|    1000501|  Silver Card|1/2/2018|        Restaurants|           debit|  24.22|   Silver Card|\n",
      "|    1000501|Platinum Card|1/3/2018|Credit Card Payment|          credit|2298.09| Platinum Card|\n",
      "|    1000501|Platinum Card|1/4/2018|      Movies & DVDs|           debit|  11.76| Platinum Card|\n",
      "|    1000501|  Silver Card|1/5/2018|        Restaurants|           debit|  25.85|   Silver Card|\n",
      "|    1000501|  Silver Card|1/6/2018|   Home Improvement|           debit|  18.45|   Silver Card|\n",
      "|    1000501|     Checking|1/8/2018|          Utilities|           debit|     45|          Cash|\n",
      "|    1000501|  Silver Card|1/8/2018|   Home Improvement|           debit|  15.38|   Silver Card|\n",
      "|    1000501|Platinum Card|1/9/2018|              Music|           debit|  10.69| Platinum Card|\n",
      "+-----------+-------------+--------+-------------------+----------------+-------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8961f-26e9-4685-b909-736a4b217713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Remove First N lines from Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7b6b7-6b50-4f7d-8820-d5a9c843b764",
   "metadata": {},
   "source": [
    "Consider we have a report of web-page traffic generated everyday which contains the analytics information such as session, pageviews, unique views etc. The sample report is shown in the figure given below. To process the data and load into Spark DataFrame, we need to remove the first 7 lines from the file, as this data is not a relevant data. Our problem statement is how will you handle this sort of files and how will you load the data into Spark DataFrame by removing first seven lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c4af8aa-537f-402e-b530-875cce5365af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "637e5365-83ad-4085-875a-4e1dd7e37462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_csv = \"https://raw.githubusercontent.com/azar-s91/dataset/master/pageview.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03e5765e-979f-4021-9545-bc1277ad33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(pageview_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "738bfc19-01eb-4aa3-9435-651599749bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file as RDD. Here we are reading with the partition as 2.\n",
    "rdd1 = sc.textFile(SparkFiles.get('pageview.csv'), 2).map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "521b2cbd-c3f9-434c-b486-75869eef0c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d136e86e-61e6-46bd-8022-b05b2a08e568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Site', 'www.learntospark.com'],\n",
       " ['Desccription', '\"Complete guide to learn Spark', 'AI', 'ML\"'],\n",
       " ['Page Views of each blog'],\n",
       " ['20200817-20200817'],\n",
       " [''],\n",
       " ['Total data in page', '12'],\n",
       " [''],\n",
       " ['Page', 'Date', 'Pageviews', 'Unique_Pageviews', 'Sessions'],\n",
       " ['Guide to Install Spark', '2020-08-17', '1140', '986', '800'],\n",
       " ['Spark MAP vs FlatMap', '2020-08-17', '836', '800', '128']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d506d75-bfcc-493c-aac0-cbc389d0c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Guide to Install Spark', '2020-08-17', '1140', '986', '800'],\n",
       " ['Spark MAP vs FlatMap', '2020-08-17', '836', '800', '128'],\n",
       " ['Spark Architechture', '2020-08-17', '1569', '1345', '1400'],\n",
       " ['Azure Function for Mp3 to text', '2020-08-17', '350', '245', '234'],\n",
       " ['Scala Vs Python', '2020-08-17', '200', '150', '130'],\n",
       " ['Spark Window Function', '2020-08-17', '789', '546', '560'],\n",
       " ['Natural Language Processing', '2020-08-17', '467', '456', '100'],\n",
       " ['Spark Linear Interpolation - Time Series',\n",
       "  '2020-08-17',\n",
       "  '698',\n",
       "  '345',\n",
       "  '349'],\n",
       " ['Spark case statement', '2020-08-17', '234', '196', '120'],\n",
       " ['Spark Scenario Based Questions', '2020-08-17', '712', '329', '137']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" We apply MapPartitionWithIndex transformation to iterate through the index of partition and remove line from 0 to 7, \n",
    "if the index is equal to 0 ie. first partition of the Spark RDD.\"\"\"\n",
    "\n",
    "rdd2 = rdd1.mapPartitionsWithIndex(lambda id_x, itr: list(itr)[8:] if id_x == 0 else itr)\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a799cd3b-6ba6-4ae7-8ad6-ede5c5ae4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+---------+------------+-------+\n",
      "|Page                                    |Date      |Pageviews|unique_views|session|\n",
      "+----------------------------------------+----------+---------+------------+-------+\n",
      "|Guide to Install Spark                  |2020-08-17|1140     |986         |800    |\n",
      "|Spark MAP vs FlatMap                    |2020-08-17|836      |800         |128    |\n",
      "|Spark Architechture                     |2020-08-17|1569     |1345        |1400   |\n",
      "|Azure Function for Mp3 to text          |2020-08-17|350      |245         |234    |\n",
      "|Scala Vs Python                         |2020-08-17|200      |150         |130    |\n",
      "|Spark Window Function                   |2020-08-17|789      |546         |560    |\n",
      "|Natural Language Processing             |2020-08-17|467      |456         |100    |\n",
      "|Spark Linear Interpolation - Time Series|2020-08-17|698      |345         |349    |\n",
      "|Spark case statement                    |2020-08-17|234      |196         |120    |\n",
      "|Spark Scenario Based Questions          |2020-08-17|712      |329         |137    |\n",
      "+----------------------------------------+----------+---------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=['Page','Date','Pageviews','unique_views','session']\n",
    "\n",
    "out_df=spark.createDataFrame(rdd2,schema)\n",
    "out_df.show(10,truncate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185d7d9-4d2e-42be-89f2-2bb8ee6387cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert Pandas DataFrame into Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98edf2-2098-494a-82b3-4d25e80541c8",
   "metadata": {},
   "source": [
    "#### Apache Arrow in Spark for Converting pandas df\n",
    "\n",
    "Apache Arrow is the open source cross-platform tool, that closely works with the in-memory columnar data formats of Spark. This provides us the ability to convert data between python process and JVM in a efficient and fast way. By default Apache Arrow is disabled in Spark and can be enabled using the below line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ae7b1ec-d1aa-4247-a6e5-e5508d6a543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable Apache Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a3f38-5fe0-49bf-9287-57e5d8f0c038",
   "metadata": {},
   "source": [
    "All Spark SQL datatype are compatible with the version of 0.10.0 of arrow and if there is any issue with the casting while conversion, then it could fall back to a non-Arrow implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e85c7289-10bf-47cc-bbab-00c9bf468627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.fallback.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09910d3d-8d00-487c-a2a3-e329dff4ab37",
   "metadata": {},
   "source": [
    "**Note:** Keep in mind that even if arrow is enabled, the conversion of Spark to pandas involves the Driver memory for collection of data into single node which make the memory overhead issue. So, the best approach is to take a sample before making conversion using toPandas() API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbb717-49e8-4fd0-99a3-1d1b6bf12c40",
   "metadata": {},
   "source": [
    "#### Cast Issues While Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92650a95-68d1-4649-8365-6f57012f73b8",
   "metadata": {},
   "source": [
    "We can convert the pandas df to Spark df using createDataFrame() API as shown below.\n",
    "\n",
    "Code Syntax to create dataframe from pandas df:\n",
    "\n",
    "`in_df=spark.createDataFrame(pandas_dataframe)`\n",
    "\n",
    "\n",
    "But if there is any datatype mismatch then while creating Spark dataframe itself it throws `TypeError` stating type mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d7039-f696-48f5-8660-9e581bc5f746",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "We can convert the Pandas DF to Spark DF in two methods. \n",
    "* By casting all the columns in pandas as string using astype() \n",
    "* By defining structType() schema and using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cc7f2-c38f-4fd0-b2d3-dc5b472f3c1f",
   "metadata": {},
   "source": [
    "##### Step 1: Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d3e8221-e0c3-4401-8f5b-365d042b6698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>Amount</th>\n",
       "      <th>discount</th>\n",
       "      <th>Member</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Arun</td>\n",
       "      <td>2020-07-15 01:01:53</td>\n",
       "      <td>2465.22</td>\n",
       "      <td>10 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005</td>\n",
       "      <td>Barath</td>\n",
       "      <td>2020-07-13 12:15:33</td>\n",
       "      <td>8399.34</td>\n",
       "      <td>5 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>Charle</td>\n",
       "      <td>2020-07-18 20:10:45</td>\n",
       "      <td>1234.88</td>\n",
       "      <td>3 %</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>Gokul</td>\n",
       "      <td>2020-07-15 11:11:36</td>\n",
       "      <td>1690.00</td>\n",
       "      <td>1 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>Messy</td>\n",
       "      <td>2020-07-18 15:11:43</td>\n",
       "      <td>160.00</td>\n",
       "      <td>3 %</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerId CustomerName             dateTime   Amount discount  Member\n",
       "0        1001         Arun  2020-07-15 01:01:53  2465.22     10 %    True\n",
       "1        1005       Barath  2020-07-13 12:15:33  8399.34      5 %    True\n",
       "2        1003       Charle  2020-07-18 20:10:45  1234.88      3 %   False\n",
       "3        1004        Gokul  2020-07-15 11:11:36  1690.00      1 %    True\n",
       "4        1005        Messy  2020-07-18 15:11:43   160.00      3 %    True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required python package\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#Read the input csv file\n",
    "in_pd=pd.read_csv('https://raw.githubusercontent.com/azar-s91/dataset/master/trans.csv')\n",
    "\n",
    "#Display sample result from pandas df\n",
    "in_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "757a070b-4dc9-45f1-ab9d-a1be97e227e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerId        int64\n",
       "CustomerName     object\n",
       "dateTime         object\n",
       "Amount          float64\n",
       "discount         object\n",
       "Member             bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36265a7-f87b-4753-b9d9-4ec83473e2f8",
   "metadata": {},
   "source": [
    "##### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1487a-8b58-4a44-ace7-22c1c0ffc040",
   "metadata": {},
   "source": [
    "###### Method 1 : Casting before conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8a2e71a-a5ad-4514-9478-b1ca57c7a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------+-------+--------+------+\n",
      "|CustomerId|CustomerName|           dateTime| Amount|discount|Member|\n",
      "+----------+------------+-------------------+-------+--------+------+\n",
      "|      1001|        Arun|2020-07-15 01:01:53|2465.22|    10 %|  True|\n",
      "|      1005|      Barath|2020-07-13 12:15:33|8399.34|     5 %|  True|\n",
      "|      1003|      Charle|2020-07-18 20:10:45|1234.88|     3 %| False|\n",
      "|      1004|       Gokul|2020-07-15 11:11:36| 1690.0|     1 %|  True|\n",
      "|      1005|       Messy|2020-07-18 15:11:43|  160.0|     3 %|  True|\n",
      "|      1006|      Gerold|2020-07-08 14:16:53| 2546.0|     1 %|  True|\n",
      "|      1007|      Parker|2020-07-04 17:13:33| 3456.0|     2 %| False|\n",
      "|      1008|        Thor|2020-07-10 03:30:43| 8745.0|     5 %|  True|\n",
      "|      1009|       Steve|2020-07-22 12:10:43|  143.0|     2 %|  True|\n",
      "|      1010|        Mani|2020-07-27 19:40:23| 1865.0|     3 %|  True|\n",
      "|      1011|      Cooper|2020-07-13 18:10:33| 1200.0|     1 %|  True|\n",
      "|      1012|       Penny|2020-07-28 13:20:33| 1693.0|     4 %| False|\n",
      "|      1013|     Lenoard|2020-07-22 10:50:33|7600.98|     1 %|  True|\n",
      "|      1014|    Mohammed|2020-07-12 07:30:33|  534.0|     2 %|  True|\n",
      "|      1015|      Althaf|2020-07-10 05:10:36| 2435.0|     1 %|  True|\n",
      "+----------+------------+-------------------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create spark df from pandas df using astype()\n",
    "in_df=spark.createDataFrame(in_pd.astype(str))\n",
    "in_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c337c02c-c882-4dd5-b692-11c012cd24f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerId: string (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- dateTime: string (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- discount: string (nullable = true)\n",
      " |-- Member: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema() used to display schema\n",
    "in_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886e3cc-daee-45ad-b5ef-10994e6560dd",
   "metadata": {},
   "source": [
    "From the result, one can observe that, we are able to make conversion from pandas df to spark df successfully. But, also notice the schema of the spark dataframe. It gives clear picture that all the column are of String datatype. If we need to perform any aggregation with numerical column in spark dataframe, then we need to again cast the column to Integer or decimal. So, the best approach would be to define the schema and try converting into spark dataframe as shown in the below step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185c751-3747-448d-8c5f-ab328a6393dd",
   "metadata": {},
   "source": [
    "###### Method 2: Define Schema and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa275155-7d84-4082-a907-f5a6109cdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema using StructType()\n",
    "from pyspark.sql.types import *\n",
    "pdsch=StructType([StructField(\"CustomerId\",IntegerType(),True),\n",
    "                  StructField(\"CustomerName\",StringType(),True),\n",
    "                  StructField(\"dateTime\",StringType(),True),\n",
    "                  StructField(\"Amount\",FloatType(),True),\n",
    "                  StructField(\"Discount\",StringType(),True),\n",
    "                  StructField(\"Member\",BooleanType(),True),\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2348190f-e892-4c23-a43d-233bd9f7c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(CustomerId,IntegerType,true),StructField(CustomerName,StringType,true),StructField(dateTime,StringType,true),StructField(Amount,FloatType,true),StructField(Discount,StringType,true),StructField(Member,BooleanType,true)))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5f20fea9-65d3-49c7-a055-af39f73b87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------+-------+--------+------+\n",
      "|CustomerId|CustomerName|           dateTime| Amount|Discount|Member|\n",
      "+----------+------------+-------------------+-------+--------+------+\n",
      "|      1001|        Arun|2020-07-15 01:01:53|2465.22|    10 %|  true|\n",
      "|      1005|      Barath|2020-07-13 12:15:33|8399.34|     5 %|  true|\n",
      "|      1003|      Charle|2020-07-18 20:10:45|1234.88|     3 %| false|\n",
      "|      1004|       Gokul|2020-07-15 11:11:36| 1690.0|     1 %|  true|\n",
      "|      1005|       Messy|2020-07-18 15:11:43|  160.0|     3 %|  true|\n",
      "|      1006|      Gerold|2020-07-08 14:16:53| 2546.0|     1 %|  true|\n",
      "|      1007|      Parker|2020-07-04 17:13:33| 3456.0|     2 %| false|\n",
      "|      1008|        Thor|2020-07-10 03:30:43| 8745.0|     5 %|  true|\n",
      "|      1009|       Steve|2020-07-22 12:10:43|  143.0|     2 %|  true|\n",
      "|      1010|        Mani|2020-07-27 19:40:23| 1865.0|     3 %|  true|\n",
      "|      1011|      Cooper|2020-07-13 18:10:33| 1200.0|     1 %|  true|\n",
      "|      1012|       Penny|2020-07-28 13:20:33| 1693.0|     4 %| false|\n",
      "|      1013|     Lenoard|2020-07-22 10:50:33|7600.98|     1 %|  true|\n",
      "|      1014|    Mohammed|2020-07-12 07:30:33|  534.0|     2 %|  true|\n",
      "|      1015|      Althaf|2020-07-10 05:10:36| 2435.0|     1 %|  true|\n",
      "+----------+------------+-------------------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe using defined schema\n",
    "in_df=spark.createDataFrame(in_pd,schema=pdsch)\n",
    "in_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec1c29b1-0a5b-4b54-bd65-a1582d0a3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- dateTime: string (nullable = true)\n",
      " |-- Amount: float (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Member: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema used to check the schema of spark df\n",
    "in_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f936fc13-1881-4f31-96fa-64c24a9cc85c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Ambiguous Column name in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f3134-b134-4025-9fe7-9518e0de3de6",
   "metadata": {},
   "source": [
    "Consider a input Spark Dataframe as shown in the above figure, which is derived from a nested JSON file. You can download the sample dataset from this link sample dataset. We can observe that there is a duplicate column name named name, our requirement is to rename any one of the duplicate or ambiguous column from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dd67c-75cd-4d08-b263-930c1a1f1cf5",
   "metadata": {},
   "source": [
    "#### Impact with Ambiguous Column in Spark\n",
    "Ambiguous column in Spark DataFrame leads to the worst impact and we will not be able to perform any transformations on top of the duplicate column as it throws the error.\n",
    "\n",
    "#### Solution\n",
    "Renaming the one of the ambiguous column name into differrent name will sort out this issue. But in Spark, we don't have a direct method to handle this use case and we need to make use of df.columns to get the duplicate columns count and index and to rename the duplicate column in Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59348da0-71eb-4111-b514-c903f45bf039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the input json file and flatten the data to replicate the use-case\n",
    "df=spark.read.json('input1.json',multiLine=True)\n",
    "df1=df.select(\"*\",\"Delivery.*\").drop(\"Delivery\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5f71e-bed3-4444-b003-5db9525360ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "df_cols=df1.columns\n",
    "\n",
    "for i in df_cols:\n",
    "    if df_cols.count(i)==2:\n",
    "        ind=df_cols.index(i)\n",
    "        lst.append(ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lst1=list(set(lst))\n",
    "for i in lst1:\n",
    "    df_cols[i]=df_cols[i]+'_0'\n",
    "\n",
    "df1=df1.toDF(*df_cols)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232edec7-b243-4494-8f1e-baccc4d53f80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Find Duplicates in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90856712-7dab-4dfe-9b9a-1bcc0cac9f5c",
   "metadata": {},
   "source": [
    "Consider we have a CSV file with some duplicate records in it as shown in the picture. Our requirement is to find duplicate records or duplicate rows in spark dataframe and report the output.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "We can solve this problem to find duplicate rows by two Method,\n",
    "* PySpark GroupBy\n",
    "* PySpark Window Rank Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2398c82f-24a4-4c10-8681-6a3bea5e4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+\n",
      "|  Name|Age|Education|Year|\n",
      "+------+---+---------+----+\n",
      "|   RAM| 28|       BE|2012|\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "| Madhu| 22|    B.Com|2018|\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "|  Bill| 32|       ME|2007|\n",
      "| Madhu| 22|    B.Com|2018|\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "|   RAM| 25|       MA|2012|\n",
      "+------+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read CSV File\n",
    "education_csv = 'data/education.csv'\n",
    "in_df=spark.read.csv(education_csv,header=True)\n",
    "in_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0336ea3-c6a7-43d4-9925-4d066eb8014d",
   "metadata": {},
   "source": [
    "#### Approach 1: GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ea0fd7e2-ef64-4360-bc78-e5c25d70f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+\n",
      "|  Name|Age|Education|Year|\n",
      "+------+---+---------+----+\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "| Madhu| 22|    B.Com|2018|\n",
      "+------+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_df.groupBy(\"Name\", \"Age\", \"Education\", \"Year\") \\\n",
    ".count() \\\n",
    ".where(\"count > 1\") \\\n",
    ".drop(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc038abe-a7b9-432d-bc1e-77e135e7da17",
   "metadata": {},
   "source": [
    "#### Approach 2: Window Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7ee2be93-ab8a-4464-acd1-a4099c12be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "07a59874-bd26-40ca-a5ee-8ba7bea24b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy(\"name\").orderBy(col(\"Year\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "72d9effb-40bd-46b7-9c2a-a4d157af33a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+---+\n",
      "|  Name|Age|Education|Year|rnk|\n",
      "+------+---+---------+----+---+\n",
      "| Madhu| 22|    B.Com|2018|  2|\n",
      "|   RAM| 25|       MA|2012|  2|\n",
      "|Rakesh| 53|      MBA|1985|  2|\n",
      "|Rakesh| 53|      MBA|1985|  3|\n",
      "+------+---+---------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all the duplicates\n",
    "in_df.withColumn(\"rnk\", row_number().over(win)).filter(\"rnk > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d2558ff9-96f7-41f0-94ce-321b3fb58d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+\n",
      "|  Name|Age|Education|Year|\n",
      "+------+---+---------+----+\n",
      "| Madhu| 22|    B.Com|2018|\n",
      "|   RAM| 25|       MA|2012|\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "+------+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Duplicates of duplicates to have unique list of records\n",
    "in_df.withColumn(\"rnk\", row_number().over(win)).filter(\"rnk > 1\").drop(\"rnk\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "98a21bd9-c640-4cbc-ba56-d3d5496bbe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+\n",
      "|  Name|Age|Education|Year|\n",
      "+------+---+---------+----+\n",
      "|  Bill| 32|       ME|2007|\n",
      "| Madhu| 22|    B.Com|2018|\n",
      "|   RAM| 28|       BE|2012|\n",
      "|Rakesh| 53|      MBA|1985|\n",
      "+------+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To have unique records based on Name column\n",
    "in_df.dropDuplicates([\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4d3aa-340e-4358-931b-d1b88da31127",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Filter Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fa21ec9f-0ff2-4094-a181-061d3d3bb8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+----+\n",
      "| Name|Age|Education|Year|\n",
      "+-----+---+---------+----+\n",
      "|  RAM| 28|       BE|2012|\n",
      "|Madhu| 22|    B.Com|2018|\n",
      "|Madhu| 22|    B.Com|2018|\n",
      "|  RAM| 25|       MA|2012|\n",
      "+-----+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter records where year > 2010\n",
    "in_df.filter(\"Year > 2010\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bc987bb8-a23e-4fbe-bbb7-85fa49f7295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+----+\n",
      "| Name|Age|Education|Year|\n",
      "+-----+---+---------+----+\n",
      "|  RAM| 28|       BE|2012|\n",
      "|Madhu| 22|    B.Com|2018|\n",
      "|Madhu| 22|    B.Com|2018|\n",
      "|  RAM| 25|       MA|2012|\n",
      "+-----+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Where is an alias for filter() method\n",
    "in_df.where(\"Year > 2010\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bc875-f797-4311-be0d-0126433b88e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Select First Row of Each Group in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f4910aa4-232a-4c8b-97e7-8fa089c047d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2e1980f7-ec97-4b1b-b3b5-29f19e16be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy(\"Name\").orderBy(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "18d40808-f2ca-4b54-be52-49c8bacd97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+---+\n",
      "|  Name|Age|Education|Year|rnk|\n",
      "+------+---+---------+----+---+\n",
      "|  Bill| 32|       ME|2007|  1|\n",
      "| Madhu| 22|    B.Com|2018|  1|\n",
      "| Madhu| 22|    B.Com|2018|  2|\n",
      "|   RAM| 28|       BE|2012|  1|\n",
      "|   RAM| 25|       MA|2012|  2|\n",
      "|Rakesh| 53|      MBA|1985|  1|\n",
      "|Rakesh| 53|      MBA|1985|  2|\n",
      "|Rakesh| 53|      MBA|1985|  3|\n",
      "+------+---+---------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_df.withColumn(\"rnk\", row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "af7347e0-83fc-49db-9e5d-ebd42958931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+----+---+\n",
      "|  Name|Age|Education|Year|rnk|\n",
      "+------+---+---------+----+---+\n",
      "|  Bill| 32|       ME|2007|  1|\n",
      "| Madhu| 22|    B.Com|2018|  1|\n",
      "|   RAM| 28|       BE|2012|  1|\n",
      "|Rakesh| 53|      MBA|1985|  1|\n",
      "+------+---+---------+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 17:27:13 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 4176902 ms exceeds timeout 120000 ms\n",
      "22/06/09 17:27:13 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "in_df.withColumn(\"rnk\", row_number().over(win)).filter(\"rnk < 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa924c4-4099-494a-8c09-fae00fa6b6e5",
   "metadata": {},
   "source": [
    "# Split and Merge Columns in Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2f827-1917-4a7e-8d81-7ee02d84600e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read MultiLine JSON in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ad017-d5ab-4452-8998-235c9aa54e0f",
   "metadata": {},
   "source": [
    "If the records in the input files are in a single line like show above, then spark.read.json will give us the expected output. If we have a single record in a multiple lines then the above command will show \"_corrupt_record\".\n",
    "\n",
    "To over come this sort of corrupted issue, we need to set multiLine parameter as True while reading the JSON file. Code snippet to do so is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4cbb245d-6b72-4362-9165-861eaa507900",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = 'data/input1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "497aa76b-03d4-473b-8efe-8d1923211e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.json(input_json, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "954d9b85-762d-4e88-8b21-83ba02354968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           Education|  name|\n",
      "+--------------------+------+\n",
      "|[{BE, 2018}, {ME,...|Clarke|\n",
      "|        [{BE, 2015}]|  John|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acd39b-1ef5-4d84-bea4-ab4721d3deb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explode Array Column\n",
    "\n",
    "convert Array of strings  i.e. Education column. We explode or flattens this column using the Spark SQL Dataframe API and SQL function explode()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c542244e-b34a-4e70-b798-7908284e51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2ec99266-f32f-4607-8a6e-9c9ff3553ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = input_df.select('name', explode('Education').alias('education_flat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e1ff2466-ae00-4963-88a6-5d88836f8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|education_flat|\n",
      "+------+--------------+\n",
      "|Clarke|    {BE, 2018}|\n",
      "|Clarke|    {ME, 2020}|\n",
      "|  John|    {BE, 2015}|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e8387c02-cba8-4292-9456-fb9101a60242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- education_flat: struct (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb84eb-0623-43b5-a0a5-807de100e5f4",
   "metadata": {},
   "source": [
    "#### Flatten Struct Columns\n",
    "\n",
    "Convert the Struct data column into two different column as Qualification and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "695f9751-0294-43dd-9ba4-06783224b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df=flat.select('name','education_flat.Qualification', 'education_flat.year' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c0b14ac6-03fc-4997-92a9-45e1c007c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----+\n",
      "|  name|Qualification|year|\n",
      "+------+-------------+----+\n",
      "|Clarke|           BE|2018|\n",
      "|Clarke|           ME|2020|\n",
      "|  John|           BE|2015|\n",
      "+------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11090e-95f2-43b5-b819-8779e36fd331",
   "metadata": {},
   "source": [
    "# Cast String Datatype to Date Timestamp in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4d663-f180-4c2d-b550-e84f13b877a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# InferSchema with StructType and StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c4349-55b7-4fdc-abf9-e9509da058c9",
   "metadata": {},
   "source": [
    "Consider we create a Spark dataframe from a CSV file which is not having a header column in it. Since the file don't have header in it, the Spark dataframe will be created with the default column names named _c0, _c1 etc. This column naming convention looks awkward and will be difficult for the developers to prepare a query statement using this column names. It will be helpful if we can create a dataframe with some meaningful column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc1920b2-0864-41cb-8b5e-72442e7b7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('data/ecom_user.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ad327040-5b1d-424a-8b5e-0455f41b40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-------+\n",
      "|    _c0|      _c1|      _c2|    _c3|\n",
      "+-------+---------+---------+-------+\n",
      "| Gaurav|  T-shirt|    Delhi|1234567|\n",
      "|Bharath|Headphone|Bangalore|5738612|\n",
      "+-------+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc716f6-6df2-445d-82a1-5d65236dba48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution to Infer / Define Schema in PySpark\n",
    "\n",
    "We can apply schema to the dataframe using StructType clause.\n",
    "\n",
    "StructType clause are used to provide schema to the Spark datframe. StructType object contains list of StructField objects that defines the name, datatype and flag to indicate null-ability. We can create schema as struct type and merge this schema with the data that we have. To do this we need to import all the sql.types and have a column list with its datatype in StructField, also have to provide nullable or not details. From StructField create StructType as shown in the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13afa925-350b-4abf-87fb-8bf79e28a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dc894ae4-77fc-40a8-85d4-08b5944a48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema fields using StructField\n",
    "data_schema=[ StructField(\"NAME\", StringType(), True),\n",
    "              StructField(\"Product\", StringType(), True),\n",
    "              StructField(\"City\", StringType(), True),\n",
    "              StructField(\"MOBILE\", StringType(), True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fa898bc7-520f-49c1-83c2-e97217f483c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(NAME,StringType,true),StructField(Product,StringType,true),StructField(City,StringType,true),StructField(MOBILE,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Create Schema using StructType which taakes StructFields as input\n",
    "struct_schema=StructType(fields=data_schema)\n",
    "print(struct_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "492a4909-8714-42ec-9e0c-081bfb21e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Schema to the DataFrame\n",
    "df1 = spark.read.csv('data/ecom_user.csv', header=False, schema=struct_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00d97530-79a3-42c1-b71e-fb9c97f68563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-------+\n",
      "|   NAME|  Product|     City| MOBILE|\n",
      "+-------+---------+---------+-------+\n",
      "| Gaurav|  T-shirt|    Delhi|1234567|\n",
      "|Bharath|Headphone|Bangalore|5738612|\n",
      "+-------+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd986e8-213d-499d-bdd0-dbc65c9ddf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2328ed-5445-46af-9e4c-c6ee1e17414c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
